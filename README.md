# ğŸ“„ LLM RAG Service (FastAPI + LangChain + ChromaDB)

This project implements a **Retrieval-Augmented Generation (RAG)** system where users can upload documents and ask contextual questions based on their content. The service uses **FastAPI** for API exposure, **LangChain** for orchestration, **ChromaDB** for vector storage, and **Google Gemini** (via `langchain-google-genai`) as the default LLM.

It is fully containerized with Docker and tested with `pytest`.

---

## âœ¨ Features

* **Upload PDFs** (up to 20 docs, 1000 pages each)
* **Smart chunking & embeddings** with `sentence-transformers`
* **Efficient vector search** using **ChromaDB**
* **Context-aware answers** generated by Gemini (LLM configurable)
* **REST API** with endpoints for upload, query, and metadata
* **Dockerized deployment** for local or cloud environments
* **Automated tests** with `pytest`

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ app
â”‚   â”œâ”€â”€ api
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ endpoints.py        # API routes (upload, query, docs)
â”‚   â”‚   â””â”€â”€ models.py           # Pydantic request/response schemas
â”‚   â”œâ”€â”€ core
â”‚   â”‚   â”œâ”€â”€ config.py           # App configuration (Pydantic Settings)
â”‚   â”‚   â””â”€â”€ rag_pipeline.py     # RAG pipeline (retrieval + LLM call)
â”‚   â”œâ”€â”€ db
â”‚   â”‚   â””â”€â”€ metadata.db         # SQLite DB for document metadata
â”‚   â”œâ”€â”€ services
â”‚   â”‚   â””â”€â”€ document_processor.py # PDF parsing, chunking, embeddings, storage
â”‚   â”œâ”€â”€ main.py                 # FastAPI entrypoint
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â””â”€â”€ Dockerfile              # App container build
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ tests
    â”œâ”€â”€ test_query.py           # Tests query pipeline
    â””â”€â”€ test_upload.py          # Tests document upload flow
```

---

## âš™ï¸ Setup & Installation

### 1) Local environment

```bash
python -m venv .venv
source .venv/bin/activate
pip install -U pip
pip install -r app/requirements.txt
```

### 2) Environment variables

Create a `.env` file:

```dotenv
APP_ENV=dev
PORT=8000

# Embeddings
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Vector DB
CHROMA_DIR=/data/chroma
CHROMA_COLLECTION=rag_docs

# LLM (Gemini)
GOOGLE_API_KEY=your_google_api_key
GENAI_MODEL=gemini-1.5-pro
GENAI_TEMPERATURE=0.2
GENAI_MAX_OUTPUT_TOKENS=1024

# Metadata DB
METADATA_DB=app/db/metadata.db
```

---

## â–¶ï¸ Run the App

**Local (dev mode):**

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

* Swagger UI: [http://localhost:8000/docs](http://localhost:8000/docs)
* ReDoc: [http://localhost:8000/redoc](http://localhost:8000/redoc)

---

## ğŸ³ Docker Deployment

**Build & run with Docker Compose:**

```bash
docker compose up --build
```

**docker-compose.yml** (already included):

```yaml
version: "3.9"
services:
  rag-api:
    build: ./app
    env_file: .env
    ports:
      - "${PORT:-8000}:8000"
    volumes:
      - rag_data:/data
    restart: unless-stopped
volumes:
  rag_data:
```

---

## ğŸ“š API Endpoints

### Health

* `GET /health` â†’ `{ "status": "ok" }`

### Documents

* `POST /documents/upload` â†’ Upload PDFs (â‰¤ 20 files, â‰¤ 1000 pages each)
* `GET /documents` â†’ List stored documents with metadata
* `GET /documents/{doc_id}` â†’ Get document details + stored metadata
* `DELETE /documents/{doc_id}` â†’ Delete document + embeddings

### Query

* `POST /query`

  * Request:

    ```json
    {
      "question": "What are the key safety findings?",
      "top_k": 5
    }
    ```
  * Response:

    ```json
    {
      "answer": "The report highlights...",
      "sources": [
        {"doc_id": "abc123", "page": 12, "snippet": "..."}
      ]
    }
    ```

---

## ğŸ”§ Key Implementation Details

* **`document_processor.py`** â†’ Extracts PDF text (`pypdf`), chunks (`langchain-text-splitters`), generates embeddings (`sentence-transformers`), stores vectors in **ChromaDB**, and updates metadata in `metadata.db`.
* **`rag_pipeline.py`** â†’ Implements retrieval pipeline: fetch relevant chunks from Chroma, build context prompt, call LLM (Gemini), and return grounded answers with citations.
* **`endpoints.py`** â†’ Defines API routes: upload documents, query system, and fetch metadata.
* **`config.py`** â†’ Centralized runtime configuration via Pydantic settings.
* **`main.py`** â†’ Initializes FastAPI app, mounts routes, loads config.

---

## ğŸ§ª Testing

Run all tests:

```bash
pytest tests -q
```

Includes:

* **Upload tests** (`test_upload.py`) â†’ verifies documents are processed and stored
* **Query tests** (`test_query.py`) â†’ verifies retrieval + LLM response (mocked)
* API tests for invalid inputs (too many docs, oversized files, empty queries)

Example (tests/test\_query.py):

```python
def test_query_returns_answer(client):
    resp = client.post("/query", json={"question": "Summarize abstract"})
    data = resp.json()
    assert resp.status_code == 200
    assert "answer" in data
    assert isinstance(data["sources"], list)
```

---

## âœ… Submission Checklist

* [x] RAG pipeline with FastAPI + LangChain
* [x] Upload + query endpoints with Chroma + Gemini
* [x] Metadata persisted in SQLite (`metadata.db`)
* [x] Dockerfile + docker-compose.yml
* [x] Automated tests with `pytest`
* [x] This README.md (documentation)

---

