# ğŸ“„ LLM RAG Service (FastAPI + LangChain + ChromaDB)

This project implements a **Retrieval-Augmented Generation (RAG)** system where users can upload documents and ask contextual questions based on their content. The service uses **FastAPI** for API exposure, **LangChain** for orchestration, **ChromaDB** for vector storage, and **Google Gemini** (via `langchain-google-genai`) as the default LLM.

It is fully containerized with Docker and tested with `pytest`.

---

## ğŸŒ Live Deployment

* **App (Render):** [https://rag-new-zjxw.onrender.com](https://rag-new-zjxw.onrender.com)
* **Swagger UI:** [https://rag-new-zjxw.onrender.com/docs](https://rag-new-zjxw.onrender.com/docs)
* **ReDoc:** [https://rag-new-zjxw.onrender.com/redoc](https://rag-new-zjxw.onrender.com/redoc)

---

## ğŸ“¦ Postman Collection

* **Public Postman Collection URL:** `https://www.postman.com/collections/ADD_YOUR_PUBLIC_COLLECTION_URL`

  > Replace `ADD_YOUR_PUBLIC_COLLECTION_URL` with your actual public link from Postman (Share â†’ Create public link).

**Recommended Postman Environment**

```json
{
  "baseUrl": "https://rag-new-zjxw.onrender.com"
}
```

For local testing set `baseUrl` to `http://localhost:8000`.

Quick requests:

* `{{baseUrl}}/health`
* `{{baseUrl}}/documents`
* `{{baseUrl}}/query`

---

## âœ¨ Features

* **Upload PDFs** (up to 20 docs, 1000 pages each)
* **Smart chunking & embeddings** with `sentence-transformers`
* **Efficient vector search** using **ChromaDB**
* **Context-aware answers** generated by Gemini (LLM configurable)
* **REST API** with endpoints for upload, query, and metadata
* **Dockerized deployment** for local or cloud environments
* **Automated tests** with `pytest`

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ app
â”‚   â”œâ”€â”€ api
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ endpoints.py        # API routes (upload, query, docs)
â”‚   â”‚   â””â”€â”€ models.py           # Pydantic request/response schemas
â”‚   â”œâ”€â”€ core
â”‚   â”‚   â”œâ”€â”€ config.py           # App configuration (Pydantic Settings)
â”‚   â”‚   â””â”€â”€ rag_pipeline.py     # RAG pipeline (retrieval + LLM call)
â”‚   â”œâ”€â”€ db
â”‚   â”‚   â””â”€â”€ metadata.db         # SQLite DB for document metadata
â”‚   â”œâ”€â”€ services
â”‚   â”‚   â””â”€â”€ document_processor.py # PDF parsing, chunking, embeddings, storage
â”‚   â”œâ”€â”€ main.py                 # FastAPI entrypoint
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â””â”€â”€ Dockerfile              # App container build
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ tests
    â”œâ”€â”€ test_query.py           # Tests query pipeline
    â””â”€â”€ test_upload.py          # Tests document upload flow
```

---

## âš™ï¸ Setup & Installation

### 1) Local environment (from source)

```bash
python -m venv .venv
source .venv/bin/activate
pip install -U pip
pip install -r app/requirements.txt
```

### 2) Environment variables

Create a `.env` file in the project root:

```dotenv
APP_ENV=dev
PORT=8000

# Embeddings
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Vector DB
CHROMA_DIR=/data/chroma
CHROMA_COLLECTION=rag_docs

# LLM (Gemini)
GOOGLE_API_KEY=your_google_api_key
GENAI_MODEL=gemini-1.5-pro
GENAI_TEMPERATURE=0.2
GENAI_MAX_OUTPUT_TOKENS=1024

# Metadata DB
METADATA_DB=app/db/metadata.db
```

---

## â–¶ï¸ Run the App

### Local (dev mode from source)

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

* Swagger UI: [http://localhost:8000/docs](http://localhost:8000/docs)
* ReDoc: [http://localhost:8000/redoc](http://localhost:8000/redoc)

---

## ğŸ³ Docker â€” Local Deployment (using Docker Hub images)

Use the published images to avoid local builds.

**Docker Hub Images**

* **ChromaDB:** `shubhamtrgupta/chroma`
* **App:** `shubhamtrgupta/rag_new`

**docker-compose.yml**

```yaml
version: "3.9"
services:
  chromadb:
    image: shubhamtrgupta/chroma
    volumes:
      - rag_data:/data
    restart: unless-stopped

  rag-api:
    image: shubhamtrgupta/rag_new
    env_file: .env
    ports:
      - "${PORT:-8000}:8000"
    depends_on:
      - chromadb
    restart: unless-stopped

volumes:
  rag_data:
```

**Run**

```bash
docker compose pull
docker compose up -d
```

* App: [http://localhost:8000](http://localhost:8000)
* Swagger UI: [http://localhost:8000/docs](http://localhost:8000/docs)

If you prefer building locally from source:

```bash
docker compose up --build
```

---

## ğŸ“š API Endpoints

### Health

* `GET /health` â†’ `{ "status": "ok" }`

### Documents

* `POST /documents/upload` â†’ Upload PDFs (â‰¤ 20 files, â‰¤ 1000 pages each)

  * Content-Type: `multipart/form-data`
  * Body: `files` (one or multiple PDF files)
* `GET /documents` â†’ List stored documents with metadata
* `GET /documents/{doc_id}` â†’ Get document details + stored metadata
* `DELETE /documents/{doc_id}` â†’ Delete document + embeddings

### Query

* `POST /query`

  * Request:

    ```json
    {
      "question": "What are the key safety findings?",
      "top_k": 5
    }
    ```
  * Response:

    ```json
    {
      "answer": "The report highlights...",
      "sources": [
        {"doc_id": "abc123", "page": 12, "snippet": "..."}
      ]
    }
    ```

---

## ğŸ§° Postman Quick Tips

* **Environment**: set `baseUrl` to the Render URL or `http://localhost:8000`.
* **DELETE with path param**:

  * Method: `DELETE`
  * URL: `{{baseUrl}}/documents/{doc_id}`
  * No body required.
* **Upload files**:

  * `POST {{baseUrl}}/documents/upload`
  * Body â†’ `form-data` â†’ key: `files` (type: *File*), select 1â€“20 PDFs.
* **Query**:

  * `POST {{baseUrl}}/query`
  * Body â†’ `raw` JSON

---

## ğŸ”§ Key Implementation Details

* **`document_processor.py`** â†’ Extracts PDF text (`pypdf`), chunks (`langchain-text-splitters`), generates embeddings (`sentence-transformers`), stores vectors in **ChromaDB**, and updates metadata in `metadata.db`.
* **`rag_pipeline.py`** â†’ Retrieval pipeline: fetch relevant chunks from Chroma, build context prompt, call LLM (Gemini), and return grounded answers with citations.
* **`endpoints.py`** â†’ API routes for upload, query, and metadata.
* **`config.py`** â†’ Centralized runtime configuration via Pydantic settings.
* **`main.py`** â†’ Initializes FastAPI app, mounts routes, loads config.

---

## ğŸ§ª Testing

Run all tests:

```bash
pytest tests -q
```

Includes:

* **Upload tests** (`tests/test_upload.py`)
* **Query tests** (`tests/test_query.py`)
* API tests for invalid inputs (too many docs, oversized files, empty queries)

**Example (`tests/test_query.py`)**

```python
def test_query_returns_answer(client):
    resp = client.post("/query", json={"question": "Summarize abstract"})
    data = resp.json()
    assert resp.status_code == 200
    assert "answer" in data
    assert isinstance(data["sources"], list)
```

---

## âœ… Submission Checklist

* [x] RAG pipeline with FastAPI + LangChain
* [x] Upload + query endpoints with Chroma + Gemini
* [x] Metadata persisted in SQLite (`metadata.db`)
* [x] Dockerfile + docker-compose.yml
* [x] Docker Hub images for local deployment (`shubhamtrgupta/chroma`, `shubhamtrgupta/rag_new`)
* [x] Render deployment link
* [x] Public Postman collection URL (replace placeholder above)
* [x] Automated tests with `pytest`
* [x] This README.md (documentation)

---

## âœ‰ï¸ Notes

* Replace the Postman collection placeholder with your actual public share link.
* Ensure `GOOGLE_API_KEY` is set in `.env` for Gemini usage.
* Chroma persistence folder (`CHROMA_DIR`) should be backed by a Docker volume for data durability.
